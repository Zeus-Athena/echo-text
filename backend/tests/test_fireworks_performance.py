#!/usr/bin/env python3
"""
Fireworks LLM æ¨¡å‹æ€§èƒ½æµ‹è¯• (å¤šé€Ÿç‡æµ‹è¯•)
æµ‹è¯•ä¸åŒè¯·æ±‚é—´éš”ä¸‹çš„é™æµæƒ…å†µ
"""

import asyncio
import statistics
import time

from openai import AsyncOpenAI

# Fireworks API é…ç½®
API_KEY = "fw_EzsCUGbkVBkiR44fozToM5"
BASE_URL = "https://api.fireworks.ai/inference/v1"

# åªæµ‹è¯• qwen3-235b-a22b-instruct-2507
MODELS = [
    "accounts/fireworks/models/qwen3-235b-a22b-instruct-2507",
]

# æµ‹è¯•æ–‡æœ¬
TEST_TEXTS = [
    "Hello everyone",
    "Today we're going to discuss",
    "the importance of machine learning",
    "in modern software development",
    "Let's start with the basics",
    "é¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹",
    "è¿™ä¸ªé¡¹ç›®çš„æ•´ä½“æ¶æ„",
    "ç„¶åå†è®¨è®ºå…·ä½“çš„å®ç°ç»†èŠ‚",
    "å¤§å®¶æœ‰ä»€ä¹ˆé—®é¢˜å¯ä»¥éšæ—¶æå‡º",
    "æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æé«˜æ•ˆç‡",
]

# ç¿»è¯‘æç¤ºè¯
SYSTEM_PROMPT = "Translate to Chinese. Output translation only, no explanation."


async def test_single_request(client: AsyncOpenAI, model: str, text: str) -> dict:
    """å•æ¬¡è¯·æ±‚æµ‹è¯•"""
    start = time.perf_counter()
    try:
        response = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": text},
            ],
            max_tokens=100,
            temperature=0.3,
        )
        latency = (time.perf_counter() - start) * 1000  # ms
        output = response.choices[0].message.content
        return {
            "success": True,
            "latency_ms": latency,
            "output": output,
        }
    except Exception as e:
        latency = (time.perf_counter() - start) * 1000
        return {"success": False, "latency_ms": latency, "error": str(e)}


async def test_model(client: AsyncOpenAI, model: str, delay: float, num_requests: int = 50) -> dict:
    """æµ‹è¯•å•ä¸ªæ¨¡å‹åœ¨ç‰¹å®šå»¶è¿Ÿä¸‹çš„è¡¨ç°"""
    print(f"\n{'=' * 60}")
    print(f"æµ‹è¯•æ¨¡å‹: {model.split('/')[-1]}")
    print(f"è¯·æ±‚é—´éš”: {delay}s | è¯·æ±‚æ¬¡æ•°: {num_requests}")
    print(f"{'=' * 60}")

    results = []
    successes = 0
    failures = 0

    # è¿ç»­é”™è¯¯è®¡æ•°å™¨ï¼Œç”¨äºæå‰ç»ˆæ­¢ä¸¥é‡é™æµçš„æµ‹è¯•
    consecutive_failures = 0

    for i in range(num_requests):
        text = TEST_TEXTS[i % len(TEST_TEXTS)]
        result = await test_single_request(client, model, text)
        results.append(result)

        if result["success"]:
            successes += 1
            consecutive_failures = 0
            if i < 3:
                print(f"  [{i + 1}] âœ… {result['latency_ms']:.0f}ms")
        else:
            failures += 1
            consecutive_failures += 1
            error_msg = result["error"]
            if "429" in error_msg:
                print(f"  [{i + 1}] âŒ é™æµ (429)")
            else:
                print(f"  [{i + 1}] âŒ {error_msg[:50]}")

            # å¦‚æœè¿ç»­å¤±è´¥è¶…è¿‡10æ¬¡ï¼Œæå‰ç»ˆæ­¢è¯¥ç»„æµ‹è¯•
            if consecutive_failures >= 10:
                print(f"\nâš ï¸ è¿ç»­å¤±è´¥ 10 æ¬¡ï¼Œæå‰ç»ˆæ­¢æœ¬ç»„æµ‹è¯• (Interval: {delay}s)")
                break

        # è¿›åº¦
        if (i + 1) % 10 == 0:
            print(f"  è¿›åº¦: {i + 1}/{num_requests}")

        # è¯·æ±‚é—´éš”
        await asyncio.sleep(delay)

    # ç»Ÿè®¡
    successful_results = [r for r in results if r["success"]]
    latencies = [r["latency_ms"] for r in successful_results]

    # fix: prevent division by zero if len(results) is 0 (though num_requests > 0, loop might break early)
    total_run = len(results)
    success_rate = (successes / total_run * 100) if total_run > 0 else 0

    stats = {
        "model": model.split("/")[-1],
        "delay": delay,
        "total_requests": total_run,
        "successes": successes,
        "failures": failures,
        "success_rate": success_rate,
        "avg_latency_ms": statistics.mean(latencies) if latencies else 0,
        "p95_latency_ms": sorted(latencies)[int(len(latencies) * 0.95)] if latencies else 0,
    }

    print(f"\nğŸ“Š é—´éš” {delay}s ç»Ÿè®¡:")
    print(f"   æˆåŠŸç‡: {stats['success_rate']:.1f}% ({successes}/{total_run})")
    print(f"   å¹³å‡å»¶è¿Ÿ: {stats['avg_latency_ms']:.0f}ms")

    return stats


async def main():
    print("=" * 60)
    print("ğŸš€ Fireworks LLM é€Ÿç‡é™åˆ¶å‹åŠ›æµ‹è¯•")
    print("=" * 60)
    print(f"API Base: {BASE_URL}")

    client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)

    # æµ‹è¯•ä¸åŒçš„å»¶è¿Ÿé—´éš”
    delays = [0.5, 1.0, 2.0]
    all_stats = []

    model = MODELS[0]  # è¿™é‡Œåªæµ‹ç¬¬ä¸€ä¸ªæ¨¡å‹ qwen3

    for delay in delays:
        try:
            stats = await test_model(client, model, delay=delay, num_requests=50)
            all_stats.append(stats)
            # ç»„é—´ä¼‘æ¯ï¼Œé˜²æ­¢ä¸Šä¸€ç»„çš„é™æµå½±å“ä¸‹ä¸€ç»„
            print("\nWaiting 5 seconds before next test...")
            await asyncio.sleep(5.0)
        except Exception as e:
            print(f"âŒ æµ‹è¯•å‡ºé”™: {e}")

    # æ±‡æ€»æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“‹ é€Ÿç‡æµ‹è¯•æŠ¥å‘Šæ±‡æ€»")
    print("=" * 60)
    print(f"{'é—´éš”(s)':<10} {'æˆåŠŸç‡':>10} {'å¹³å‡å»¶è¿Ÿ':>12} {'å»ºè®®':>10}")
    print("-" * 60)

    for stats in all_stats:
        rec = "âœ… å¯ç”¨" if stats["success_rate"] > 95 else "âŒ é™æµ"
        print(
            f"{stats['delay']:<10.1f} "
            f"{stats['success_rate']:>9.1f}% "
            f"{stats['avg_latency_ms']:>10.0f}ms "
            f"{rec:>10}"
        )

    print("-" * 60)


if __name__ == "__main__":
    asyncio.run(main())
